<!DOCTYPE html>
<html lang="en"><head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Markdown with Math Rendered</title>

    <!-- Load Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600&amp;family=Roboto+Mono:wght@400;600&amp;display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/bootstrap/3.2.0/css/bootstrap.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/highlight.js/9.1.0/styles/github.min.css">

    <style>

    /* General Styles */
    body {
        font-family: 'Inter', sans-serif;
        background-color: #f8f9fa;
        color: #333;
        margin: 0;
        padding: 40px;
        line-height: 1.6;
        display: flex;
        justify-content: center;
        font-size: 19.2px; 
    }

    /* Content Box */
    .container {
        max-width: 1000px;
        background: white;
        padding: 30px;
        border-radius: 10px;
        box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.1);
    }

    /* Headings */
    h1 {
        font-size: 33.6px; 
        font-weight: 600;
        color: #2c3e50;
        margin-bottom: 15px;
    }

    h2 {
        font-size: 28.8px; 
        font-weight: 600;
        color: #2c3e50;
        margin-bottom: 15px;
    }

    h3 {
        font-size: 24px; 
        font-weight: 600;
        color: #2c3e50;
        margin-bottom: 15px;
    }

    ul {
        padding-left: 20px;
        font-size: 19.2px; 
    }

    ul li {
        margin-bottom: 8px;
    }

    pre {
        font-size: 18px; 
    }

    code {
        font-size: 18px;
    }
    </style>
</head>
<body>
    <div class="container">
        <h1>Reinforcement Learning - Summary</h1>
<h2>Exact Technical Implementation Details</h2>
<h3>Markov Decision Process (MDP) Model</h3>
<ol>
<li>
<p><strong>States (<code>S</code>)</strong>: A list of possible states an agent can be in.</p>
</li>
<li>
<p><strong>Actions (<code>A</code>)</strong>: A list of possible actions an agent can take.</p>
</li>
<li>
<p><strong>Transition Function (<code>T</code>)</strong>: This function defines the probabilities of moving between states after taking an action. The form is <code>T(s, a, s')</code> which gives the probability of transitioning to state <code>s'</code> from state <code>s</code> by taking action <code>a</code>.</p>
</li>
<li>
<p><strong>Reward Function (<code>R</code>)</strong>: A function <code>R(s, a)</code> that provides the immediate reward received after executing action <code>a</code> in state <code>s</code>. This can be a constant or an expectation of a possible distribution if random.</p>
</li>
<li>
<p><strong>Discount Factor (<code>γ</code>)</strong>: A value between 0 and 1 that reduces future rewards' importance compared to immediate rewards.</p>
</li>
<li>
<p><strong>Horizon (<code>H</code>)</strong>: Specifies the number of decisions to be made, which can be finite (<code>N</code>) or infinite (<code>∞</code>).</p>
</li>
</ol>
<h3>Python Implementation for MDP</h3>
<pre><code class="language-python">from typing import List, Dict

# Define the components of MDP
S: List[str] = []  # List of states
A: List[str] = []  # List of actions
T: Dict[str, Dict[str, Dict[str, float]]] = {}  # T[s][a][s'] = P(s' | s, a)
R: Dict[str, Dict[str, float]] = {}  # R[s][a] = reward for taking action a in state s
γ: float = 0.9  # Discount factor
H: int = 100  # Horizon (use a large number for infinite horizon)

# Example state transition and rewards
S = ['s1', 's2', 's3']
A = ['a1', 'a2']
T = {
    's1': {
        'a1': {'s1': 0.7, 's2': 0.3},
        'a2': {'s2': 0.4, 's3': 0.6}
    },
    's2': {
        'a1': {'s2': 0.9, 's1': 0.1},
        'a2': {'s3': 1.0}
    },
    's3': {
        'a1': {'s3': 1.0},
        'a2': {'s1': 0.5, 's2': 0.5}
    }
}
R = {
    's1': {'a1': 5, 'a2': 10},
    's2': {'a1': 3, 'a2': 4},
    's3': {'a1': 2, 'a2': 7}
}
</code></pre>
<h3>Value Functions and Policy Iteration</h3>
<ul>
<li>
<p><strong>Value Function (<code>vπ</code>)</strong>: Represents the expected return when starting from state <code>s</code> and following policy <code>π</code>.</p>
</li>
<li>
<p><strong>Policy Improvement</strong>: Modify the policy by choosing actions that maximize the expected value.</p>
</li>
</ul>
<h4>Python Code for Value Iteration</h4>
<pre><code class="language-python">import numpy as np

def compute_value_iteration(S, A, T, R, γ, θ=0.0001):
    v = {s: 0 for s in S}  # Initial value function
    while True:
        delta = 0
        for s in S:
            # Keep track of the highest value change
            v_old = v[s]
            v[s] = max(
                sum(T[s][a][s_prime] * (R[s][a] + γ * v[s_prime]) for s_prime in S)
                for a in A
            )
            delta = max(delta, abs(v[s] - v_old))
        if delta &lt; θ:
            break
    return v

# Compute value iteration for the defined MDP
v_optimal = compute_value_iteration(S, A, T, R, γ)
</code></pre>
<h3>Policy Iteration</h3>
<h4>Python Code for Policy Evaluation and Improvement</h4>
<pre><code class="language-python">def policy_iteration(S, A, T, R, γ):
    policy = {s: np.random.choice(A) for s in S}  # Initial random policy
    v = {s: 0 for s in S}  # Initial value function

    def policy_evaluation(policy, v, θ=0.0001):
        while True:
            delta = 0
            for s in S:
                v_old = v[s]
                a = policy[s]
                v[s] = sum(
                    T[s][a][s_prime] * (R[s][a] + γ * v[s_prime])
                    for s_prime in S
                )
                delta = max(delta, abs(v[s] - v_old))
            if delta &lt; θ:
                break

    is_policy_stable = False
    while not is_policy_stable:
        policy_evaluation(policy, v)
        is_policy_stable = True
        for s in S:
            old_action = policy[s]
            policy[s] = max(
                A, key=lambda a: sum(T[s][a][s_prime] * (R[s][a] + γ * v[s_prime]) for s_prime in S)
            )
            if old_action != policy[s]:
                is_policy_stable = False
    return policy, v

# Run policy iteration
optimal_policy, optimal_value = policy_iteration(S, A, T, R, γ)
</code></pre>
<h2>Value-Based vs. Policy Search Approaches</h2>
<h3>Value-Based Approaches</h3>
<ul>
<li><strong>Value Iteration</strong>: Computes the value function for each state; derived policy is based on this function.</li>
<li><strong>Policy Iteration</strong>: Iterates between evaluating the current policy and improving it.</li>
</ul>
<h3>Policy Search Approaches</h3>
<ul>
<li>Directly searches for the optimal policy by optimizing performance, sometimes involving parameterized policies (e.g., neural networks).</li>
</ul>
<h3>Actor-Critic Methods</h3>
<ul>
<li>Combines both policy search and value-based methods. An actor is responsible for policy updates, and a critic evaluates the policy.</li>
</ul>
<h2>Mathematical Concepts Implementation as Python Code</h2>
<h3>Q-Learning (Off-Policy)</h3>
<h4>Python Code for Q-Learning</h4>
<pre><code class="language-python">def q_learning(S, A, T, R, γ, α=0.1, ε=0.1, episodes=1000):
    Q = {s: {a: 0 for a in A} for s in S}  # Initialize Q-table

    for episode in range(episodes):
        s = np.random.choice(S)  # Random initial state
        while True:
            if np.random.rand() &lt; ε:
                a = np.random.choice(A)  # Exploration: random action
            else:
                a = max(A, key=lambda x: Q[s][x])  # Exploitation: best action
            
            # Simulate transition
            s_prime = np.random.choice(S, p=[T[s][a][next_s] for next_s in S])
            r = R[s][a]

            # Update Q value
            Q[s][a] += α * (r + γ * max(Q[s_prime].values()) - Q[s][a])

            if s_prime is a terminal state or some condition:
                break
            s = s_prime

    return Q

# Execute Q-learning
optimal_Q = q_learning(S, A, T, R, γ)
</code></pre>
<p>This summary provides a detailed explanation of the technical aspects of reinforcement learning, focusing on the implementation of MDP and related reinforcement learning concepts such as value iteration, policy iteration, and Q-learning. Each step and component has been converted into clear and executable Python code with ample commentary for understanding.</p>
<p><a href="https://arxiv.org/abs/2005.14419">View original paper</a></p>

    </div>


</body></html>